<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <link rel="shortcut icon" href="./static/assets/newsish-fav.png">
  <title>News-ish</title>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">

  <!-- D3 Import -->
  <script src="https://d3js.org/d3.v5.min.js"></script>   

  <!-- Our CSS -->
  <link rel="stylesheet" type="text/css" href="../static/css/style.css">

</head>

<body>
  <div class="container">
    <div class="row">
        <div class="col-4"></div>
        <div class="col-1 menu-right"><a href="about">about</a></div>
        <div class="col-2">
            <h3 class="header-row"><a href="/">News-<i>ish</i></h3></a>
        </div>
        <div class="col-1 menu-right"><a href="team">team</a></div>
    </div>
    <hr>
</div>   

<div class="container">
    <div class="row">
        <!-- PROJECT DESCRIPTION SECTION -->
        <div class="col-12" id="project-description-section"><h2 class="centered">Project Description</h2><hr></div>
        <div class="col-12"><h6 class="article thin">&emsp;Our project is a machine learning exercise in which we used a Naïve Bayes classification model to determine whether a news article is considered a reliable source of information. We utilized Natural Language Processing, Naïve Bayes & Confusion Matrix to build our model to predict these news sources as “unreliable” or “reliable”. Using Naïve Bayes, we determined which words in these articles were more commonly found in unreliable versus reliable, this was the base for our model to make predictions. We utilized WordCloud to visualize the words used in the determined “unreliable” and “reliable” articles to see which topics are common targets of both determinations. Related pairs of words can also be used in scatterplots to show relationships between frequencies and the division between determined “reliable” and “unreliable” articles in the dataset. Our website features a user input search bar on our home page in which you can input your own article to determine whether it is a reliable information source.</h6><br></div>
        
        
        <!-- NAÏVE BAYES SECTION -->
        <div class="col-12" id="naive-bayes-section"><h2 class="centered">Naïve Bayes Model</h2><hr></div>
        <div class="col-12"><h6 class="article thin">&emsp;Naïve Bayes is a supervised machine learning classifier. Naïve Bayes is notedfor being a fast algorithm and being fairly accurate with predicting outcomes but works very well predicting natural language processing (NLP) problems, our project is a NLP problem. We used Naïve Bayes to predict whether or not a news article can be classified as “reliable” or “unreliable” based on certain texts pertaining to their respective “tag words”. Naïve Bayes combines both probability and Bayes’ Theorem to predict the outcome of a text, then categorizes it to a tag word. A good example of Naïve Bayes classification is categorizing emails into “Primary” or “Spam” inboxes based on the text of the email. To put Naïve Bayes simply, “tag words” is synonymous with “categories” and we are trying to decipher snippets of text that can be put into these categories. For the texts, we used the title of the article, the body of the article and a combination of both the title and body of the article to explore the different outcomes and accuracy.</h6><br></div>

            <!-- MATRIX SECTION -->
        <div class="col-12" id="matrix-section"><h2 class="centered">Confusion Matrix</h2><hr></div>
        <div class="col-12"><h6 class="article thin">A confusion matrix is a way of testing the performance of a classification algorithm, in our case, we used the confusion matrix to summarize whether our Naïve Bayes classification was performing the way we wanted. We know that from our dataset that there are two classes: “reliable” and “unreliable”, so we used the confusion matrix for a binary classifier, as there are only two predicted classes. Since there are two classes for our project, the 4 quadrants of the confusion matrix model are as follows: True Positives (TP), True Negatives (TN), False Positives (FP) & False Negatives (FN). <br><br> The quadrants are described as: 
            <ul>
                <li>True Positives (TP): News articles we predicted as “reliable” and they turned out to be “reliable”</li>
                <li>True Negatives (TN): News articles we predicted as “unreliable” and they turned out to be “unreliable”</li>
                <li>False Positives (FP): News articles we predicted as “reliable” and they turned out to be “unreliable”</li>
                <li>False Negatives (FN): News articles we predicted as “unreliable” and they turned out to be “reliable”</li>
            </ul>
            </h6>
        </div>

        <div class="col-md-6">
                <img class="centered-img-sbs" src="../static/assets/trainingdata-matrix.jpg" alt="training data matrix">
                <h5 class="centered padding">Matrix : Training Data</h5>
        </div>
        <div class="col-md-6">
                <img class="centered-img-sbs" src="../static/assets/testdata-matrix.jpg" alt="test data matrix">
                <h5 class="centered padding">Matrix : Test Data</h5>
        </div>

        <div class="col-md-12">
            <br>
            <img class="centered-img padding" src="../static/assets/matrix-desc.jpg" alt="matrix description">
        </div>

        <div class="col-12"><h6 class="article thin"><br>&emsp;Following the training data, the model correctly identified 7,300 “reliable” and 5,500 “unreliable” news items out of approximately 12,518 in the set.  With the test data, the model predicted 3,100 “reliable” and 2,100 “unreliable” news items out of approximately 5,500 in the set.<br><br>
            <ul>
                <li>Precision: 99% of predictions of “unreliable” news were accurate, while 92% of predictions of “reliable” news were accurate.<br></li>
                <li>Recall: 99% of “reliable” items were correctly predicted, while 89% of “unreliable” items were correctly predicted.<br></li>
                <li>F1 Score: The harmonic average of Precision and Recall provided an F1 Score of 95%.<br></li>
                <li>Accuracy: Overall, 95% of the predictions made were correct.</li>
            </ul>
                    <br></h6></div>

        <!-- VISUALIZING SECTION -->
        <div class="col-12" id="word-cloud-section"><h2 class="centered">Visualizing the Data</h2><hr></div>
        <div class="col-6">
            <img class="centered-img" src="../static/assets/reliable.jpg" alt="reliable word cloud">
            <h5 class="centered padding">Reliable Word Cloud</h5>
        </div>
        <div class="col-6">
            <img class="centered-img" src="../static/assets/unreliable.jpg" alt="unreliable word cloud">
            <h5 class="centered padding">Unreliable Word Cloud</h5>
        </div>
        

        <div class="col-12">
            <br><br>
            <h3 class="centered">Data</h3>
            <hr>
            <h6 class="centered"><a style="color: rgb(11, 135, 207)" href="https://www.kaggle.com/c/fake-news/data" target="_blank">kaggle.com | FAKE-NEWS</a></h6>
            <hr>
         </div>

    </div>
</div>

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>


</body>

</html>
